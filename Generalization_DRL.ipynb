{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ReCovNet: Deep Reinforcement Learning for Solving MBCLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare: Install dependencies\n",
    "### Install with pip\n",
    "* python=3.7\n",
    "* PyTorch>=1.1\n",
    "* numpy\n",
    "* tqdm\n",
    "* cv2\n",
    "* tensorboard_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils import torch_load_cpu, load_problem, get_inner_model, move_to\n",
    "from nets.attention_model import AttentionModel\n",
    "from train import train_epoch, validate\n",
    "from tensorboard_logger import Logger as TbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Namespace(baseline=None, batch_size=640, bl_alpha=0.05, bl_warmup_epochs=0, checkpoint_encoder=False, checkpoint_epochs=1, data_distribution=None, device=device(type='cuda'), embedding_dim=128, epoch_size=128000, epoch_start=0, eval_batch_size=1000, eval_only=False, exp_beta=0.8, hidden_dim=128, load_path=None, log_dir='logs', log_step=50, lr_critic=0.0001, lr_decay=1, lr_model=0.0001, max_grad_norm=1.0, model='attention', n_encode_layers=3, n_epochs=500, n_facilities=100, n_users=1109, no_cuda=False, no_progress_bar=False, no_tensorboard=False, normalization='batch', output_dir='outputs', p=20, problem='MCLP', r=None, resume=None, run_name='100_20_20230906T211530', save_dir='outputs\\\\MCLP\\\\100_20_20230906T211530', seed=2023, shrink_size=None, tanh_clipping=10.0, use_cuda=True, val_dataset=None, val_size=2000)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the run args\n",
    "%run options\n",
    "\n",
    "# Set the random seed\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Optionally configure tensorboard\n",
    "tb_logger = None\n",
    "if not opts.no_tensorboard:\n",
    "    tb_logger = TbLogger(os.path.join(opts.log_dir, \"{}_{}\".format(opts.problem, opts.n_users, opts.n_facilities), opts.run_name))\n",
    "\n",
    "# Set the device\n",
    "use_cuda=True\n",
    "opts.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "opts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Figure out what's the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "problems.MCLP.problem_MCLP.MCLP"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem = load_problem(opts.problem)\n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Initialize our policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "AttentionModel(\n  (init_embed): Linear(in_features=2, out_features=128, bias=True)\n  (init_dynamic): Linear(in_features=1, out_features=32, bias=True)\n  (l2_dynamic): Linear(in_features=32, out_features=64, bias=True)\n  (l3_dynamic): Linear(in_features=64, out_features=128, bias=True)\n  (embedder): GraphAttentionEncoder(\n    (layers): Sequential(\n      (0): MultiHeadAttentionLayer(\n        (0): SkipConnection(\n          (module): MultiHeadAttention()\n        )\n        (1): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): SkipConnection(\n          (module): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=128, bias=True)\n          )\n        )\n        (3): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): MultiHeadAttentionLayer(\n        (0): SkipConnection(\n          (module): MultiHeadAttention()\n        )\n        (1): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): SkipConnection(\n          (module): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=128, bias=True)\n          )\n        )\n        (3): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): MultiHeadAttentionLayer(\n        (0): SkipConnection(\n          (module): MultiHeadAttention()\n        )\n        (1): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): SkipConnection(\n          (module): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=128, bias=True)\n          )\n        )\n        (3): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n  (gru): GRU(128, 128, batch_first=True)\n  (project_node_embeddings): Linear(in_features=128, out_features=384, bias=False)\n  (project_fixed_context): Linear(in_features=128, out_features=128, bias=False)\n  (project_step_context): Linear(in_features=256, out_features=128, bias=False)\n  (project_out): Linear(in_features=128, out_features=128, bias=False)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = {\n",
    "    # 'pointer': PointerNetwork,\n",
    "    'attention': AttentionModel\n",
    "}.get(opts.model, None)\n",
    "\n",
    "assert model_class is not None, \"Unknown model: {}\".format(model_class)\n",
    "model = model_class(\n",
    "    opts.embedding_dim,\n",
    "    opts.hidden_dim,\n",
    "    problem,\n",
    "    n_encode_layers=opts.n_encode_layers,\n",
    "    mask_inner=True,\n",
    "    mask_logits=True,\n",
    "    normalization=opts.normalization,\n",
    "    tanh_clipping=opts.tanh_clipping,\n",
    "    checkpoint_encoder=opts.checkpoint_encoder,\n",
    "    shrink_size=opts.shrink_size,\n",
    "    dy=False\n",
    ").to(opts.device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [*] Loading the trained model from ./output/epoch-200.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts.load_path = './output/epoch-200.pt'\n",
    "# load model from load_path\n",
    "assert opts.load_path is None or opts.resume is None, \"Only one of load path and resume can be given\"\n",
    "load_path = opts.load_path if opts.load_path is not None else opts.resume\n",
    "if load_path is not None:\n",
    "    print('  [*] Loading the trained model from {}'.format(load_path))\n",
    "    load_data = torch_load_cpu(load_path)\n",
    "\n",
    "# Overwrite model parameters by parameters to load q\n",
    "model_ = get_inner_model(model)\n",
    "model.load_state_dict({**model_.state_dict(), **load_data.get('model', {})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Synthetic dada n=2000, m=1000, p=15, r=0.15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import random\n",
    "n_users = 4000\n",
    "n_facilities = 2000\n",
    "n_centers = 15\n",
    "radius = 0.15\n",
    "users = [(random.random(), random.random()) for i in range(n_users)]\n",
    "facilities = [(random.random(), random.random()) for i in range(n_facilities)]\n",
    "demand = np.random.randint(1, 2, size=n_users)\n",
    "users, facilities = np.array(users), np.array(facilities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def gen_random_data(num_sample):\n",
    "    random_datasets = []\n",
    "    for i in range(num_sample):\n",
    "        random_data = {}\n",
    "        random_data[\"users\"] = torch.tensor(users).to(torch.float32)\n",
    "        random_data[\"facilities\"] = torch.tensor(np.array(facilities)).to(torch.float32)\n",
    "        random_data['demand'] = torch.tensor(demand).to(torch.float32)\n",
    "        random_data[\"p\"] = 15\n",
    "        random_data[\"r\"] = 0.15\n",
    "        random_datasets.append(random_data)\n",
    "    return random_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "num_sample = 1\n",
    "opts.eval_batch_size = 10\n",
    "opts.max_calc_batch_size = 1280000\n",
    "width = 64\n",
    "real_datasets = gen_random_data(num_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "opts.decode_strategy = 'sampling'\n",
    "model.eval()\n",
    "model.set_decode_type(\n",
    "    \"greedy\" if opts.decode_strategy in ('bs', 'greedy') else \"sampling\")\n",
    "dataloader = DataLoader(real_datasets, batch_size=opts.eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_best(sequences, cost, ids=None, batch_size=None):\n",
    "    \"\"\"\n",
    "    Ids contains [0, 0, 0, 1, 1, 2, ..., n, n, n] if 3 solutions found for 0th instance, 2 for 1st, etc\n",
    "    :param sequences:\n",
    "    :param lengths:\n",
    "    :param ids:\n",
    "    :return: list with n sequences and list with n lengths of solutions\n",
    "    \"\"\"\n",
    "    if ids is None:\n",
    "        idx = cost.argmin()\n",
    "        return sequences[idx:idx+1, ...], cost[idx:idx+1, ...]\n",
    "\n",
    "    splits = np.hstack([0, np.where(ids[:-1] != ids[1:])[0] + 1])\n",
    "    mincosts = np.minimum.reduceat(cost, splits)\n",
    "\n",
    "    group_lengths = np.diff(np.hstack([splits, len(ids)]))\n",
    "    all_argmin = np.flatnonzero(np.repeat(mincosts, group_lengths) == cost)\n",
    "    result = np.full(len(group_lengths) if batch_size is None else batch_size, -1, dtype=int)\n",
    "\n",
    "    result[ids[all_argmin[::-1]]] = all_argmin[::-1]\n",
    "\n",
    "    return [sequences[i] if i >= 0 else None for i in result], [cost[i] if i >= 0 else math.inf for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The objective of MCBLP by DRL is: 3672.0\n",
      "The running time of DRL is: 0.5417745113372803\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "results = []\n",
    "for batch in tqdm(dataloader, disable=True):\n",
    "    batch = move_to(batch, opts.device)\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        if opts.decode_strategy in ('sampling', 'greedy'):\n",
    "            if opts.decode_strategy == 'greedy':\n",
    "                assert width == 0, \"Do not set width when using greedy\"\n",
    "                assert opts.eval_batch_size <= opts.max_calc_batch_size, \\\n",
    "                    \"eval_batch_size should be smaller than calc batch size\"\n",
    "                batch_rep = 1\n",
    "                iter_rep = 1\n",
    "            elif width * opts.eval_batch_size > opts.max_calc_batch_size:\n",
    "                assert opts.eval_batch_size == 1\n",
    "                assert width % opts.max_calc_batch_size == 0\n",
    "                batch_rep = opts.max_calc_batch_size\n",
    "                iter_rep = width // opts.max_calc_batch_size\n",
    "            else:\n",
    "                batch_rep = width\n",
    "                iter_rep = 1\n",
    "            assert batch_rep > 0\n",
    "            # This returns (batch_size, iter_rep shape)\n",
    "\n",
    "            sequences, costs = model.sample_many(batch, batch_rep=batch_rep, iter_rep=iter_rep)\n",
    "            batch_size = len(costs)\n",
    "            ids = torch.arange(batch_size, dtype=torch.int64, device=costs.device)\n",
    "#         else:\n",
    "#             # assert opts.decode_strategy == 'bs'\n",
    "\n",
    "#             cum_log_p, sequences, costs, ids, batch_size = model.beam_search(\n",
    "#                 batch, beam_size=width,\n",
    "#                 compress_mask=opts.compress_mask,\n",
    "#                 max_calc_batch_size=opts.max_calc_batch_size\n",
    "#             )\n",
    "            if sequences is None:\n",
    "                sequences = [None] * batch_size\n",
    "                costs = [math.inf] * batch_size\n",
    "            else:\n",
    "                sequences, costs = get_best(\n",
    "                    sequences.cpu().numpy(), costs.cpu().numpy(),\n",
    "                    ids.cpu().numpy() if ids is not None else None,\n",
    "                    batch_size\n",
    "                )\n",
    "            duration = time.time() - start\n",
    "            for seq, cost in zip(sequences, costs):\n",
    "                seq = seq.tolist()\n",
    "                results.append((cost, seq, duration))\n",
    "costs, tours, durations = zip(*results)\n",
    "print(f\"The objective of MCBLP by DRL is: {-costs[0]}\")\n",
    "end = time.time()-start \n",
    "print(f\"The running time of DRL is: {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from Algorithm.GA import GeneticAlgorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "dist = np.sum((facilities[:, np.newaxis, :] - users[np.newaxis, :, :]) ** 2, axis=-1) ** 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current top solution: [1978, 529, 1187, 1353, 452, 154, 1556, 1098, 670, 1816, 1361, 320, 626, 1080, 605] f=1160\n",
      "\n",
      "Final top solution: [534, 428, 1191, 1959, 648, 1677, 800, 246, 136, 1020, 1019, 1152, 1069, 504, 1668] f=611\n",
      "Time: 00:00:3.4500\n",
      "The Set of centers are: [534, 428, 1191, 1959, 648, 1677, 800, 246, 136, 1020, 1019, 1152, 1069, 504, 1668]\n",
      "The objective is: 3389\n"
     ]
    }
   ],
   "source": [
    "genetic = GeneticAlgorithm(n_users, n_facilities, n_centers, dist, radius, demand)\n",
    "genetic.optimize()\n",
    "obj = np.sum(demand) - genetic.top_chromosome.fitness\n",
    "centers = genetic.top_chromosome.content\n",
    "\n",
    "print(\"The Set of centers are: %s\" % centers)\n",
    "print(\"The objective is: %s\" % str(round(obj)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}